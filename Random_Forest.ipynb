{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "-Python imports\n",
    "\n",
    "-Train-test split\n",
    "\n",
    "-Decision Trees\n",
    "\n",
    "-Measures of node impurity (Shannon Entropy and Gini Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df['label'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[data.feature_names], \n",
    "    df['label'], \n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build a decision tree model to review their structure before moving on to random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "mdl = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=data.feature_names,  \n",
    "                   class_names=data.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "predicted_labels = dt.predict(X_test)\n",
    "plot_confusion_matrix(dt, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    scores = [accuracy_score, recall_score, precision_score, f1_score]\n",
    "    s_labels = ['Accuracy', 'Recall', 'Precision', 'F1']\n",
    "    for score, s_label in zip(scores, s_labels):\n",
    "        if s_label == 'Accuracy':\n",
    "            print(s_label + ': ' + str(score(y_test, y_pred)))\n",
    "        else:\n",
    "            print(s_label + ': ' + str(score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Knowledge Check\n",
    "1. Are decision trees deterministic?\n",
    "1. How are decision trees split determined?\n",
    "1. Are decision trees parametric? \n",
    "1. Decision trees often have high variance, why might that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Knowledge Check Answers\n",
    "1. Are decision trees deterministic?\n",
    "    - Yes they are deterministic. The best split will be found at each iterative step and will be used. \n",
    "1. How are decision trees split determined?\n",
    "    - Information gain or entropy reduction\n",
    "1. Are decision trees parametric? \n",
    "    - No, they are not parametric. Splits may differ in direction based on values.\n",
    "1. Decision trees often have high variance, why might that be?\n",
    "    - May split on wrong features or overfit to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Expanding on Decision Trees\n",
    "1. How might decision trees be leveraged to reduce variance?\n",
    "    1. Create multiple classifiers and average the results. Multiple weak learners can ofter produce a strong learner.\n",
    "1. Would multiple deterministic decision trees be useful?\n",
    "    1. Only if they were NOT deterministic.\n",
    "1. How could they not be deteministic?\n",
    "    1. Bootstrapping data and limiting features at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "-An ensemble method that combines many decision trees which have been given different subsets of the data and features to create a strong learner. \n",
    "\n",
    "-Reduces variance and creates a non-deterministic model\n",
    "\n",
    "-Generally use a large number of bushy trees\n",
    "\n",
    "-Can get excellent performance with minimum tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import = pd.DataFrame()\n",
    "ft_import['Features'] = data.feature_names\n",
    "ft_import['Importance'] = rf.feature_importances_\n",
    "ft_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import.plot.bar(x='Features', y='Importance', rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and evaluation\n",
    "\n",
    "1. Which model performed better?\n",
    "2. Which feature had the most influence on the random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Objectives \n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "1. Tune the random forest model\n",
    "    1. Number of estimators\n",
    "    1. Criterion: default is gini, can also try entropy\n",
    "    1. Max depth, min samples\n",
    "    1. Number of features\n",
    "1. Create a random forest regressor and test on sklearn Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
