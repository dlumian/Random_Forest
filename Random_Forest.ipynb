{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use the Iris flower dataset from sklearn to introduce classification with Random Forest.\n",
    "\n",
    "Feature importance and partial dependency plots will be created once the model is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "-Python imports\n",
    "\n",
    "-Train-test split\n",
    "\n",
    "-Classification metrics\n",
    "\n",
    "-Decision Trees\n",
    "\n",
    "-Measures of node impurity (Shannon Entropy and Gini Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df['label'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[data.feature_names], \n",
    "    df['label'], \n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build a decision tree model to review their structure before moving on to random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "mdl = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=data.feature_names,  \n",
    "                   class_names=data.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "predicted_labels = dt.predict(X_test)\n",
    "plot_confusion_matrix(dt, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    scores = [accuracy_score, recall_score, precision_score, f1_score]\n",
    "    s_labels = ['Accuracy', 'Recall', 'Precision', 'F1']\n",
    "    for score, s_label in zip(scores, s_labels):\n",
    "        if s_label == 'Accuracy':\n",
    "            print(s_label + ': ' + str(score(y_test, y_pred)))\n",
    "        else:\n",
    "            print(s_label + ': ' + str(score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Knowledge\n",
    "1. Are decision trees deterministic?\n",
    "    - Yes they are deterministic. The best split will be found at each iterative step and will be used. \n",
    "1. How are decision trees split determined?\n",
    "    - Information gain or entropy reduction\n",
    "1. Are decision trees parametric? \n",
    "    - No, they are not parametric. Splits may differ in direction based on values.\n",
    "1. Decision trees often have high variance, why might that be?\n",
    "    - May split on wrong features or overfit to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Expanding on Decision Trees\n",
    "1. How might decision trees be leveraged to reduce variance?\n",
    "    1. Create multiple classifiers and average the results. Multiple weak learners can ofter produce a strong learner.\n",
    "1. Would multiple deterministic decision trees be useful?\n",
    "    1. Only if they were NOT deterministic.\n",
    "1. How could they not be deteministic?\n",
    "    1. Bootstrapping data and limiting features at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "-An ensemble method that combines many decision trees which have been given different subsets of the data and features to create a strong learner. \n",
    "\n",
    "-Reduces variance and creates a non-deterministic model\n",
    "\n",
    "-Generally use a large number of bushy trees\n",
    "\n",
    "-Can get excellent performance with minimum tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing trees in the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(est_num=0):\n",
    "    fn=data.feature_names\n",
    "    cn=data.target_names\n",
    "    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "    tree.plot_tree(rf.estimators_[est_num],\n",
    "                   feature_names = fn, \n",
    "                   class_names=cn,\n",
    "                   filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(est_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(est_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing only on the first split in the two trees above, we can see differences in the splits used to build the forest.\n",
    "\n",
    "The first one split on sepal lenth <= 5.35 and the second on petal length <=2.45. The depth of the trees also varies. This is due to the randomness induced in the trees with bootstrapping and feature selection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import = pd.DataFrame()\n",
    "ft_import['Features'] = data.feature_names\n",
    "ft_import['Importance'] = rf.feature_importances_\n",
    "ft_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All importances sum to 1\n",
    "ft_import.Importance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_import.plot.bar(x='Features', y='Importance', rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependency Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the marginal effect of a feature on predictions.\n",
    "\n",
    "Shows effect of predictions when all observations have a feature set to a particular value.\n",
    "\n",
    "Relationship may be linear or complex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "for target in data.target_names:\n",
    "    print('Target is: ', target)\n",
    "    plot_partial_dependence(rf, X_train, data.feature_names, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of Random Forests\n",
    "\n",
    "1. Ensemble model (Wisdom of the Crowd)\n",
    "1. Good out-of-box performance\n",
    "1. Multiple trees can be trained at once "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cons of Random Forests\n",
    "1. Expensive to train\n",
    "2. Can produce very large model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and evaluation\n",
    "\n",
    "1. Which model performed better?\n",
    "2. Which feature had the most influence on the random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Objectives \n",
    "1. Apply a random forest classifier to a dataset\n",
    "1. Visualize feature importances from a trained random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Check\n",
    "1. A random forest is the same as combining many decision trees?\n",
    "1. Name two ways in which random forests are made non-deterministic. \n",
    "1. Random forest classifiers are parametric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "1. Tune the random forest classifier\n",
    "    1. Number of estimators\n",
    "    1. Criterion: default is gini, can also try entropy\n",
    "    1. Max depth, min samples\n",
    "    1. Number of features\n",
    "1. Create a random forest regressor and test on sklearn Boston housing data\n",
    "    1. Compare to decision tree model\n",
    "    1. Create partial dependency plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Day Activities\n",
    "1. Code random forest from scratch\n",
    "1. Code partial dependecy plot function from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
